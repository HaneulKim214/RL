{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "administrative-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "from mab import ABn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "endangered-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frame(object):\n",
    "    \"\"\"\n",
    "    Instantiate Frame object either combi or stationary with their CTR\n",
    "    \"\"\"\n",
    "    def __init__(self, ctr, name):\n",
    "        self.ctr = ctr\n",
    "        self.name = name\n",
    "    \n",
    "    def display_frame(self):\n",
    "        \"\"\"% of time frame object will be displayed to potential customers\n",
    "        reward = 1 if click else 0\n",
    "        \n",
    "        # Note that ctr can we replaced with diff wilson_ctr\n",
    "        \"\"\"\n",
    "        reward = np.random.binomial(n=1, p=self.ctr)\n",
    "        return reward\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'frame name = {self.name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reserved-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "Frame1 = Frame(0.004, \"frme1\")\n",
    "Frame2 = Frame(0.016, \"frme2\")\n",
    "Frame3 = Frame(0.02, \"frme3\")\n",
    "Frame4 = Frame(0.028, \"frme4\")\n",
    "Frame5 = Frame(0.031, \"frme5\")\n",
    "frames = [Frame1, Frame2, Frame3, Frame4, Frame5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-conversion",
   "metadata": {},
   "source": [
    "# A/B/n test\n",
    "\n",
    "$Q_{n+1} = Q_{n} + {1\\over n}(R_{n} - Q_{n})$ \n",
    "\n",
    "where $Q_{n} = {R_{1} +R_{2} + ... + R_{n-1} \\over n-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 10000\n",
    "n_prod = 50000\n",
    "\n",
    "n_frames = len(frames)\n",
    "q_values = np.zeros(n_frames)\n",
    "imps = np.zeros(n_frames) # total impressions\n",
    "total_reward = 0\n",
    "avg_reward = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_test):\n",
    "    fr_chosen = np.random.randint(n_frames)\n",
    "    reward = frames[fr_chosen].display_frame()\n",
    "    \n",
    "    imps[fr_chosen] += 1\n",
    "    q_values[fr_chosen] += (1/imps[fr_chosen]) * (reward - q_values[fr_chosen])\n",
    "\n",
    "    total_reward += reward\n",
    "    avg_reward_so_far = total_reward/(i+1)\n",
    "    avg_reward.append(avg_reward_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now showing best frame \"all the time\"\n",
    "best_frame = frames[np.argmax(q_values)]\n",
    "for i in range(n_prod):\n",
    "    reward = best_frame.display_frame()\n",
    "    total_reward += reward\n",
    "    \n",
    "    avg_reward_so_far = total_reward/(n_test + i + 1)\n",
    "    avg_reward.append(avg_reward_so_far)\n",
    "    \n",
    "history[\"ABn\"] = avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-invalid",
   "metadata": {},
   "source": [
    "###  Using class example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "abn_method = ABn(\"A/B/n method\", frames)\n",
    "abn_method.run_test(n_test =10000)\n",
    "\n",
    "print(f\"best frame = {abn_method.actions[np.argmax(abn_method.q_values)]}\")\n",
    "print(f\"{abn_method.best_a}\")\n",
    "\n",
    "abn_method.run_prod(n_prod=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-great",
   "metadata": {},
   "source": [
    "# e-greedy\n",
    "\n",
    "Q-update function is same as A/B/n test however two main differences\n",
    "\n",
    "1. one additional hyperparameter $\\epsilon$\n",
    "2. Choose action with highest q_value from the start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_prod = 60000\n",
    "eps = 0.1\n",
    "\n",
    "n_frames = len(frames)\n",
    "q_values = np.zeros(n_frames)\n",
    "imps = np.zeros(n_frames)\n",
    "total_reward = 0\n",
    "avg_reward = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_chosen = np.random.randint(n_frames)\n",
    "\n",
    "for i in range(n_prod):\n",
    "    reward = frames[fr_chosen].display_frame()\n",
    "    \n",
    "    imps[fr_chosen] += 1\n",
    "    q_values[fr_chosen] += (1/imps[fr_chosen]) * (reward - q_values[fr_chosen])\n",
    "    \n",
    "    total_reward += reward\n",
    "    avg_reward_so_far = total_reward/(i + 1)\n",
    "    avg_reward.append(avg_reward_so_far)\n",
    "    \n",
    "    if np.random.uniform() <= eps:\n",
    "        fr_chosen = np.random.randint(n_frames)\n",
    "    else:\n",
    "        fr_chosen = np.argmax(q_values)\n",
    "        \n",
    "history[\"e-greedy\"] = avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-gentleman",
   "metadata": {},
   "source": [
    "# Upper Confidence Bounds (UCB)\n",
    "\n",
    "$A_{t} = \\underset{a}{\\operatorname{argmax}}[Q_{t}(a) + c \\sqrt{\\ln t \\over N_{t}(a)}]$\n",
    "\n",
    "where $uncertainty = \\sqrt{\\ln t \\over N_{t}(a)}$\n",
    "\n",
    "- c = hyperparameter that tunes uncertanty measure.\n",
    "- q_values gets updated in same way as ABn & e-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "other-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_prod = 60000\n",
    "c = 0.1\n",
    "\n",
    "n_frames = len(frames)\n",
    "q_values = np.zeros(n_frames)\n",
    "imps = np.zeros(n_frames)\n",
    "total_reward = 0\n",
    "avg_reward = []\n",
    "\n",
    "fr_indices = np.array(range(n_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "color-wisdom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_prod):\n",
    "    \n",
    "    if any(imps==0): # randomly choose from frames with NO impressions \n",
    "        fr_chosen = np.random.choice(fr_indices[imps==0])\n",
    "    else:\n",
    "        uncertainty = np.sqrt(np.log(i+1) / imps)\n",
    "        fr_chosen = np.argmax(q_values + c*uncertainty)\n",
    "        \n",
    "    reward = frames[fr_chosen].display_frame()\n",
    "    imps[fr_chosen] += 1\n",
    "    q_values[fr_chosen] += (1/imps[fr_chosen]) * (reward - q_values[fr_chosen])\n",
    "    \n",
    "    total_reward += reward\n",
    "    avg_reward_so_far = total_reward/(i + 1)\n",
    "    avg_reward.append(avg_reward_so_far)\n",
    "    \n",
    "history[\"UCB\"] = avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-motel",
   "metadata": {},
   "source": [
    "# Thompson sampling\n",
    "\n",
    "$p(\\theta_{k}) = {{\\tau (\\alpha_{k} + \\beta_{k})} \\over {\\tau (\\alpha_{k}) \\tau(\\beta_{k}) }} \\theta_{k}^{\\alpha_{k-1}}(1-\\theta_{k})^{\\beta_{k} - 1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-religious",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-visitor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-sacrifice",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-subscriber",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = hist_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for method in methods:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = hist_df.index,\n",
    "            y = hist_df[method],\n",
    "            name = method\n",
    "        )\n",
    "    )\n",
    "    \n",
    "fig.update_layout(title=\"<b>Comparative methods for MABs</b>\",\n",
    "                  xaxis_title = \"n_prod\",\n",
    "                  yaxis_title = \"Avg. Reward\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-drain",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
